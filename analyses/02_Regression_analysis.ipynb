{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8f2aa2",
   "metadata": {},
   "source": [
    "#  Regression analysis\n",
    "\n",
    "* **Project:** ADRD Genetic Diversity in Biobanks\n",
    "* **Version:** Python/3.10\n",
    "* **Last Updated:** 28-October-2024\n",
    "\n",
    "## Notebook Overview\n",
    "Using logistic regression to analyze the protective and conditional models, Create covariate files for r2 and interaction models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a7c28",
   "metadata": {},
   "source": [
    "## Variables used \n",
    "`${ancestry}` = EUR, AFR, AMR, EAS, SAS, AAC, MDE, AJ, FIN, CAS, CAH\n",
    "\n",
    "`chr${}`:Position:A1:A2= Chromosom's number, position, reference and alternative alleles\n",
    "\n",
    "`${chr}` = 2, 19, 7, 21, 4, 11, 14, 20, 15, 16, 17\n",
    "\n",
    "`${APOE}`= e4 carriers, e4e4 carriers, e3e3 carriers\n",
    "\n",
    "`${APOE genotype}`= e3/e4, e4/e4, e1/e4, e3/e3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7021c",
   "metadata": {},
   "source": [
    "### Using logistic regression to analyze the protective model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33288c8c",
   "metadata": {},
   "source": [
    "#### Extract variants and generate VCF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda634f9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from functools import reduce\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4dc8da55",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Define variations\n",
    "variations = [chr${}:position:A1:A2]\n",
    "\n",
    "# Define file directory\n",
    "file_dir = \"${WORK_DIR}\"\n",
    "\n",
    "# Output VCF file\n",
    "output_vcf = \"${WORK_DIR}/extracted_variations.vcf\"\n",
    "\n",
    "# Open the output VCF file for writing\n",
    "with open(output_vcf, \"w\") as vcf_file:\n",
    "    # Write the VCF file header\n",
    "    vcf_file.write(\"##fileformat=VCFv4.2\\n\")\n",
    "    vcf_file.write(\"#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\n\")\n",
    "    \n",
    "    # Iterate over each variation\n",
    "    for variation in variations:\n",
    "        # Extract chromosome from variation\n",
    "        chromosome, pos, ref, alt = variation.split(\":\")\n",
    "        # Construct file path\n",
    "        file_path = f\"{file_dir}{chromosome}.compact_filtered.r4.wgs.biallelic.pvar\"\n",
    "        # Check if file exists\n",
    "        if os.path.exists(file_path):\n",
    "            # Grep for variation in file and capture the output\n",
    "            result = subprocess.run([\"grep\", \"-e\", variation, file_path], capture_output=True, text=True)\n",
    "            if result.stdout:\n",
    "                # Split the output line into fields\n",
    "                fields = result.stdout.strip().split()\n",
    "                # Ensure there are enough fields in the output\n",
    "                if len(fields) >= 5:\n",
    "                    # Extract relevant VCF fields from the output\n",
    "                    chrom = fields[0]\n",
    "                    pos = fields[1]\n",
    "                    id = fields[2]\n",
    "                    ref = fields[3]\n",
    "                    alt = fields[4]\n",
    "                    # Write the variation to the VCF file\n",
    "                    vcf_file.write(f\"{chrom}\\t{pos}\\t{id}\\t{ref}\\t{alt}\\t.\\t.\\t.\\n\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d56cc22",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%writefile var_keep.txt\n",
    "chr${}:position:A1:A2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7e6613d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%bash\n",
    "plink2 --pfile ${WORK_DIR}/chr${}.compact_filtered.r4.wgs.biallelic \\\n",
    "--extract var_keep.txt --make-bed --out ${WORK_DIR}/vars_chr${}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "879540bb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%bash\n",
    "plink2 --bfile ${WORK_DIR}/vars_chr${} \\\n",
    "--recode vcf-iid --out ${WORK_DIR}/vars_chr${}_vcf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0258610a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%bash\n",
    "module load bcftools\n",
    "\n",
    "# Define the directory path\n",
    "dir_path=\"${WORK_DIR}\"\n",
    "\n",
    "# Compress the VCF files with bgzip\n",
    "for file in vars_chr2_vcf.vcf vars_chr19_vcf.vcf vars_chr7_vcf.vcf vars_chr21_vcf.vcf vars_chr4_vcf.vcf vars_chr11_vcf.vcf vars_chr14_vcf.vcf vars_chr20_vcf.vcf vars_chr15_vcf.vcf vars_chr16_vcf.vcf vars_chr17_vcf.vcf; do\n",
    "    bgzip -c $dir_path/$file > $dir_path/${file}.gz\n",
    "done\n",
    "\n",
    "# Index the compressed VCF files\n",
    "for file in vars_chr2_vcf.vcf vars_chr19_vcf.vcf vars_chr7_vcf.vcf vars_chr21_vcf.vcf vars_chr4_vcf.vcf vars_chr11_vcf.vcf vars_chr14_vcf.vcf vars_chr20_vcf.vcf vars_chr15_vcf.vcf vars_chr16_vcf.vcf vars_chr17_vcf.vcf; do\n",
    "    bcftools index $dir_path/${file}.gz\n",
    "done\n",
    "\n",
    "# Merge the compressed VCF files with --force-samples\n",
    "bcftools merge --force-samples \\\n",
    "$dir_path/vars_chr2_vcf.vcf.gz \\\n",
    "$dir_path/vars_chr19_vcf.vcf.gz \\\n",
    "$dir_path/vars_chr7_vcf.vcf.gz \\\n",
    "$dir_path/vars_chr21_vcf.vcf.gz \\\n",
    "$dir_path/vars_chr4_vcf.vcf.gz \\\n",
    "$dir_path/vars_chr11_vcf.vcf.gz \\\n",
    "$dir_path/vars_chr14_vcf.vcf.gz \\\n",
    "$dir_path/vars_chr20_vcf.vcf.gz \\\n",
    "$dir_path/vars_chr15_vcf.vcf.gz \\\n",
    "$dir_path/vars_chr16_vcf.vcf.gz \\\n",
    "$dir_path/vars_chr17_vcf.vcf.gz \\\n",
    "-o $dir_path/merged.vcf.gz\n",
    "\n",
    "# Index the merged VCF file\n",
    "bcftools index $dir_path/merged.vcf.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3fe33",
   "metadata": {},
   "source": [
    "#### Create a covariate file including principal components (PCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee10559",
   "metadata": {},
   "source": [
    "##### Generate PCs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63c9ed94",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%bash\n",
    "plink2 --pfile ${WORK_DIR}/FILTERED.merged_biallelic_${ancestry} --maf 0.05 --geno 0.02 --hwe 0.00001 --make-bed --out ${WORK_DIR}/QC_unrelated_FILTERED.merged_biallelic_${ancestry} --remove ${WORK_DIR}/REMOVE.FILTERED.merged_biallelic_${ancestry}.related --threads 20 \n",
    "plink2 --bfile  QC_unrelated_FILTERED.merged_biallelic_${ancestry} --indep-pairwise 50 10 0.1 --out QC_unrelated_FILTERED.merged_biallelic_${ancestry}_prune_variants\n",
    "## Merge any ancestries that cannot be processed in the prevous step due to insufficient sample size with the closest related ancestry\n",
    "plink2 --bfile QC_unrelated_FILTERED.merged_biallelic_FIN --bmerge QC_unrelated_FILTERED.merged_biallelic_MDE.bed QC_unrelated_FILTERED.merged_biallelic_MDE.bim QC_unrelated_FILTERED.merged_biallelic_MDE.fam --make-bed --out merged_FIN_MDE\n",
    "plink2 --bfile QC_unrelated_FILTERED.merged_biallelic_EUR --bmerge merged_FIN_MDE.bed merged_FIN_MDE.bim merged_FIN_MDE.fam --make-bed --out QC_unrelated_FILTERED.merged_biallelic_EUR_FIN_MDE\n",
    "plink2 --bfile QC_unrelated_FILTERED.merged_biallelic_EUR_FIN_MDE --indep-pairwise 50 10 0.1 --out QC_unrelated_FILTERED.merged_biallelic_EUR_FIN_MDE_prune_variants\n",
    "\n",
    "plink2 --bfile QC_unrelated_FILTERED.merged_biallelic_${ancestry} --extract QC_unrelated_FILTERED.merged_biallelic_${ancestry}_prune_variants.prune.in --pca --out QC_unrelated_FILTERED.merged_biallelic_${ancestry}_PCA --threads 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd310dd",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file names to merge\n",
    "file_names = [\n",
    "    \"{WORK_DIR}/QC_unrelated_FILTERED.merged_biallelic_AAC_PCA.eigenvec\",\n",
    "    \"{WORK_DIR}/QC_unrelated_FILTERED.merged_biallelic_AFR_PCA.eigenvec\",\n",
    "    \"{WORK_DIR}/QC_unrelated_FILTERED.merged_biallelic_AJ_PCA.eigenvec\",\n",
    "    \"{WORK_DIR}/QC_unrelated_FILTERED.merged_biallelic_AMR_PCA.eigenvec\",\n",
    "    \"{WORK_DIR}/QC_unrelated_FILTERED.merged_biallelic_CAH_PCA.eigenvec\",\n",
    "    \"{WORK_DIR}/QC_unrelated_FILTERED.merged_biallelic_CAS_PCA.eigenvec\",\n",
    "    \"{WORK_DIR}/QC_unrelated_FILTERED.merged_biallelic_EAS_PCA.eigenvec\",\n",
    "    \"{WORK_DIR}/QC_unrelated_FILTERED.merged_biallelic_EUR_FIN_MDE_PCA.eigenvec\",\n",
    "    \"{WORK_DIR}/QC_unrelated_FILTERED.merged_biallelic_SAS_PCA.eigenvec\"\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each file and read it, ignoring the header\n",
    "for file_name in file_names:\n",
    "    df = pd.read_csv(file_name, delim_whitespace=True, comment='#', header=None,\n",
    "                     names=[\"#FID\", \"IID\", \"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \"PC9\", \"PC10\"])\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a file\n",
    "merged_df.to_csv('All_PCS_ADSP_Correct.eigenvec', sep='\\t', index=False)\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b67f1",
   "metadata": {},
   "source": [
    "##### Add PCs to the covariate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d518e14c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the covars_for_QC.txt file\n",
    "covars_df = pd.read_csv(\"{WORK_DIR}/covars_for_QC.txt\", sep=\"\\t\")\n",
    "\n",
    "# Load the All_PCS_ADSP_Correct.eigenvec file with specified column names\n",
    "pca_df = pd.read_csv(\"{WORK_DIR}/All_PCS_ADSP_Correct.eigenvec\", sep=\"\\t\", names=[\"#FID\", \"IID\"] + [f\"PC{i}\" for i in range(1, pca_df.shape[1]-1)])\n",
    "\n",
    "# Rename '#FID' to 'FID' in the pca_df\n",
    "pca_df.rename(columns={\"#FID\": \"FID\"}, inplace=True)\n",
    "\n",
    "# Merge the two dataframes on 'FID' and 'IID'\n",
    "merged_df = pd.merge(covars_df, pca_df, on=['FID', 'IID'], how='inner')\n",
    "\n",
    "# Save the merged dataframe to a new file\n",
    "merged_df.to_csv(\"{WORK_DIR}/covars_alldata_PCA.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffd2df",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the covars_for_QC.txt file\n",
    "covars_df = pd.read_csv(\"{WORK_DIR}/covars_for_QC.txt\", sep=\"\\t\")\n",
    "\n",
    "# Replace 'NA' with '-999' in the 'AGE' and 'RACE' columns\n",
    "covars_df['AGE'] = covars_df['AGE'].fillna(-999)\n",
    "covars_df['RACE'] = covars_df['RACE'].fillna(-999)\n",
    "\n",
    "# Load the All_PCS_ADSP_Correct.eigenvec file to determine the number of columns\n",
    "pca_temp_df = pd.read_csv(\"{WORK_DIR}/All_PCS_ADSP_Correct.eigenvec\", sep=\"\\t\")\n",
    "num_pcs = pca_temp_df.shape[1] - 2  # Subtract 2 for the first two columns '#FID' and 'IID'\n",
    "\n",
    "# Load the All_PCS_ADSP_Correct.eigenvec file with specified column names\n",
    "pca_df = pd.read_csv(\"{WORK_DIR}/All_PCS_ADSP_Correct.eigenvec\", sep=\"\\t\", names=[\"#FID\", \"IID\"] + [f\"PC{i}\" for i in range(1, num_pcs + 1)])\n",
    "\n",
    "# Rename '#FID' to 'FID' in the pca_df\n",
    "pca_df.rename(columns={\"#FID\": \"FID\"}, inplace=True)\n",
    "\n",
    "# Merge the two dataframes on 'FID' and 'IID'\n",
    "merged_df = pd.merge(covars_df, pca_df, on=['FID', 'IID'], how='inner')\n",
    "\n",
    "# Save the merged dataframe to a new file\n",
    "merged_df.to_csv(\"{WORK_DIR}/covars_alldata_with999forAGRandRACE_PCA.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e17af",
   "metadata": {},
   "source": [
    "#### Create PHENO files for each ancestry"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bedb8954",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%bash\n",
    "module load plink/2.0\n",
    "\n",
    "plink2 --pfile ${WORK_DIR}/FILTERED.merged_biallelic_${ancestry} \\\n",
    "--remove ${WORK_DIR}/REMOVE.FILTERED.merged_biallelic_${ancestry}.related \\\n",
    "--make-bed \\\n",
    "--out /${WORK_DIR}/FID_IID_${ancestry}_unrelated"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3dc2c43",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your .fam file\n",
    "fam_file_path = '{WORK_DIR}/FID_IID_{ancestry}_unrelated.fam'\n",
    "\n",
    "# Read the .fam file into a pandas DataFrame without headers\n",
    "df = pd.read_csv(fam_file_path, delim_whitespace=True, header=None)\n",
    "\n",
    "# Add headers to the DataFrame\n",
    "df.columns = ['FID', 'IID', 'PAT', 'MAT', 'SEX', 'PHENO1']\n",
    "\n",
    "# Save the DataFrame back to the same file with headers\n",
    "df.to_csv(fam_file_path, sep=' ', index=False, header=True)\n",
    "\n",
    "# Print the first few lines of the updated file to confirm\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3ab36a9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your .fam file\n",
    "fam_file_path = '{WORK_DIR}/FID_IID_{ancestry}_unrelated.fam'\n",
    "\n",
    "# Read the .fam file into a pandas DataFrame\n",
    "df = pd.read_csv(fam_file_path, delim_whitespace=True)\n",
    "\n",
    "# Select only the columns 'FID', 'IID', 'PHENO1'\n",
    "df_new = df[['FID', 'IID', 'PHENO1']]\n",
    "\n",
    "# Define the path for the new file\n",
    "new_file_path = '{WORK_DIR}/FID_IID_PHENO_{ancestry}.fam'\n",
    "\n",
    "# Save the new DataFrame to a new file\n",
    "df_new.to_csv(new_file_path, sep=' ', index=False)\n",
    "\n",
    "print(f\"The new file with columns 'FID', 'IID', 'PHENO1' has been saved to: {new_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055886c",
   "metadata": {},
   "source": [
    "#### Logistic regression for protective model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3625b66",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%bash\n",
    "module load plink/2.0\n",
    "\n",
    "# Define VCF file\n",
    "vcf_file=\"${WORK_DIR}/vars_${chr}_vcf.vcf.gz\"\n",
    "\n",
    "# Define an array of ancestries\n",
    "ancestries=(\"EUR\" \"AFR\" \"AMR\" \"EAS\" \"SAS\" \"AAC\" \"MDE\" \"AJ\" \"FIN\" \"CAS\" \"CAH\")\n",
    "\n",
    "# Loop over each ancestry\n",
    "for ancestry in \"${ancestries[@]}\"; do\n",
    "  echo \"Processing ancestry: $ancestry\"\n",
    "\n",
    "  # Run PLINK\n",
    "  plink2 \\\n",
    "    --vcf \"$vcf_file\" \\\n",
    "    --double-id \\\n",
    "    --pheno \"${WORK_DIR}/FID_IID_PHENO_${ancestry}.fam\" \\\n",
    "    --adjust \\\n",
    "    --ci 0.95 \\\n",
    "    --covar \"${WORK_DIR}/covars_alldata_with999forAGRandRACE_PCA.txt\" \\\n",
    "    --covar-name SEX,AGE,PC1,PC2,PC3,PC4,PC5 \\\n",
    "    --threads 15 \\\n",
    "    --covar-variance-standardize \\\n",
    "    --out \"${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_${chr}\" \\\n",
    "    --glm omit-ref firth-fallback cols=+a1freq,+a1freqcc,+a1count,+totallele,+a1countcc,+totallelecc,+gcountcc,+err \\\n",
    "    --silent\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47e742",
   "metadata": {},
   "source": [
    "#### Preparing a table for the results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5506bcdd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%bash\n",
    "# Define the output file name\n",
    "output_file=\"${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_All_variants.txt\"\n",
    "\n",
    "# Combine the files into one\n",
    "cat ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr11.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr14.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr15.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr16.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr17.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr19.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr20.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr21.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr2.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr4.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr7.PHENO1.glm.logistic.hybrid > \"$output_file\"\n",
    "\n",
    "echo \"Combined file created: $output_file\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306ae33",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "dir_path = \"{WORK_DIR}\"\n",
    "\n",
    "# Define the list of files and their corresponding ancestry codes\n",
    "files_and_ancestries = [\n",
    "    (\"Logistic_FID_IID_PHENO_case_controls_AAC_All_variants.txt\", \"AAC\"),\n",
    "    (\"Logistic_FID_IID_PHENO_case_controls_AFR_All_variants.txt\", \"AFR\"),\n",
    "    (\"Logistic_FID_IID_PHENO_case_controls_AMR_All_variants.txt\", \"AMR\"),\n",
    "    (\"Logistic_FID_IID_PHENO_case_controls_AJ_All_variants.txt\", \"AJ\"),\n",
    "    (\"Logistic_FID_IID_PHENO_case_controls_EUR_All_variants.txt\", \"EUR\"),\n",
    "    (\"Logistic_FID_IID_PHENO_case_controls_CAS_All_variants.txt\", \"CAS\"),\n",
    "    (\"Logistic_FID_IID_PHENO_case_controls_SAS_All_variants.txt\", \"SAS\"),\n",
    "    (\"Logistic_FID_IID_PHENO_case_controls_MDE_All_variants.txt\", \"MDE\"),\n",
    "    (\"Logistic_FID_IID_PHENO_case_controls_EAS_All_variants.txt\", \"EAS\"),\n",
    "    (\"Logistic_FID_IID_PHENO_case_controls_FIN_All_variants.txt\", \"FIN\"),\n",
    "    (\"Logistic_FID_IID_PHENO_case_controls_CAH_All_variants.txt\", \"CAH\"),\n",
    "    \n",
    "]\n",
    "\n",
    "# Process each file\n",
    "for file_name, ancestry in files_and_ancestries:\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "    # Add the new column with the corresponding ancestry code\n",
    "    df['ancestry'] = ancestry\n",
    "\n",
    "    # Save the updated DataFrame to a new file\n",
    "    output_file_name = file_name.replace(\".txt\", \"_with_ancestry.txt\")\n",
    "    output_file_path = os.path.join(dir_path, output_file_name)\n",
    "    df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "    print(f\"Updated file saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb574a8d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "dir_path = f\"{WORK_DIR}\"\n",
    "\n",
    "# List of file paths to be combined\n",
    "file_paths = [\n",
    "    \"Logistic_FID_IID_PHENO_case_controls_EAS_All_variants_with_ancestry.txt\",\n",
    "    \"Logistic_FID_IID_PHENO_case_controls_SAS_All_variants_with_ancestry.txt\",\n",
    "    \"Logistic_FID_IID_PHENO_case_controls_CAS_All_variants_with_ancestry.txt\",\n",
    "    \"Logistic_FID_IID_PHENO_case_controls_EUR_All_variants_with_ancestry.txt\",\n",
    "    \"Logistic_FID_IID_PHENO_case_controls_AMR_All_variants_with_ancestry.txt\",\n",
    "    \"Logistic_FID_IID_PHENO_case_controls_AAC_All_variants_with_ancestry.txt\",\n",
    "    \"Logistic_FID_IID_PHENO_case_controls_AFR_All_variants_with_ancestry.txt\",\n",
    "    \"Logistic_FID_IID_PHENO_case_controls_AJ_All_variants_with_ancestry.txt\",\n",
    "    \"Logistic_FID_IID_PHENO_case_controls_MDE_All_variants_with_ancestry.txt\",\n",
    "    \"Logistic_FID_IID_PHENO_case_controls_CAH_All_variants_with_ancestry.txt\",\n",
    "    \"Logistic_FID_IID_PHENO_case_controls_FIN_All_variants_with_ancestry.txt\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to hold the DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Read each file and append the DataFrame to the list\n",
    "for file_name in file_paths:\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new file\n",
    "output_file_path = os.path.join(dir_path, \"Combined_Logistic_with_ancestry.hybrid\")\n",
    "combined_df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Combined data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf4cbd",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined data file\n",
    "file_path = f\"{WORK_DIR}/Combined_Logistic_with_ancestry.hybrid\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Filter the rows where the TEST column has the value \"ADD\"\n",
    "filtered_df = df[df['TEST'] == 'ADD']\n",
    "\n",
    "# Save the filtered DataFrame to a new file\n",
    "output_file_path = f\"{WORK_DIR}/Filtered_Combined_Logistic_with_ancestry.hybrid\"\n",
    "filtered_df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616d26f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the filtered data file\n",
    "file_path = f\"{WORK_DIR}/Filtered_Combined_Logistic_with_ancestry.hybrid\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Select only the required columns\n",
    "columns_to_keep = ['#CHROM', 'POS', 'REF', 'ALT', 'A1', 'P', 'OR', 'L95', 'U95', 'ancestry']\n",
    "filtered_df = df[columns_to_keep]\n",
    "\n",
    "# Save the filtered DataFrame to a new file\n",
    "output_file_path = f\"{WORK_DIR}/Selected_Columns_Filtered_Combined_Logistic_with_ancestry.hybrid\"\n",
    "filtered_df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Data with selected columns saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43609f37",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data file\n",
    "file_path = f\"{WORK_DIR}/Selected_Columns_Filtered_Combined_Logistic_with_ancestry.hybrid\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Define the desired order of ancestries\n",
    "ancestry_order = ['EUR', 'AFR', 'AMR', 'EAS', 'SAS', 'MDE', 'AJ', 'FIN', 'AAC', 'CAS', 'CAH']\n",
    "\n",
    "# Initialize a list to hold the rows for the new table\n",
    "combined_data = []\n",
    "\n",
    "# Group the data by variation (defined by #CHROM, POS, REF, ALT)\n",
    "grouped = df.groupby(['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "\n",
    "# Iterate over each group and format the data for the combined table\n",
    "for (chrom, pos, ref, alt), group in grouped:\n",
    "    # Append the variation row\n",
    "    combined_data.append([f\"{chrom}:{pos} {ref}>{alt}\", \"\", \"\", \"\"])\n",
    "    \n",
    "    # Append the header row for ancestries\n",
    "    combined_data.append([\"Ancestry\", \"A1\", \"P\", \"OR (L95_U95)\"])\n",
    "    \n",
    "    # Create a dictionary of ancestries and their data\n",
    "    ancestry_dict = {row['ancestry']: [row['A1'], row['P'], f\"{row['OR']} ({row['L95']}_{row['U95']})\"] for _, row in group.iterrows()}\n",
    "    \n",
    "    # Append the data rows for each ancestry in the specified order if it exists\n",
    "    for ancestry in ancestry_order:\n",
    "        if ancestry in ancestry_dict:\n",
    "            combined_data.append([ancestry] + ancestry_dict[ancestry])\n",
    "\n",
    "    # Add an empty row for separation between variations\n",
    "    combined_data.append([\"\", \"\", \"\", \"\"])\n",
    "\n",
    "# Convert the combined data into a DataFrame\n",
    "combined_df = pd.DataFrame(combined_data)\n",
    "\n",
    "# Save the combined DataFrame to a new file\n",
    "output_file_path = \"Combined_Variations_Table.tsv\"\n",
    "combined_df.to_csv(output_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "print(f\"Combined variations table saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f874970",
   "metadata": {},
   "source": [
    "### Using logistic regression to analyze the Conditional models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ddde4",
   "metadata": {},
   "source": [
    "#### Create three covariate files that include APOE status for e4 carriers, e4/e4 carriers, and e3/e3 carriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fe47ca",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "input_dir = \"{WORK_DIR}\" \n",
    "output_dir = \"{WORK_DIR}\" \n",
    "\n",
    "# List of files to process (Check the 00_ADSP notebook for generating these files)\n",
    "files = [\n",
    "    \"adsp_vars_aac_apoe_unrelated_recode_test.APOE_GENOTYPES.csv\",\n",
    "    \"adsp_vars_afr_apoe_unrelated_recode_test.APOE_GENOTYPES.csv\",\n",
    "    \"adsp_vars_aj_apoe_unrelated_recode_test.APOE_GENOTYPES.csv\",\n",
    "    \"adsp_vars_amr_apoe_unrelated_recode_test.APOE_GENOTYPES.csv\",\n",
    "    \"adsp_vars_cah_apoe_unrelated_recode_test.APOE_GENOTYPES.csv\",\n",
    "    \"adsp_vars_cas_apoe_unrelated_recode_test.APOE_GENOTYPES.csv\",\n",
    "    \"adsp_vars_eas_apoe_unrelated_recode_test.APOE_GENOTYPES.csv\",\n",
    "    \"adsp_vars_eur_apoe_unrelated_recode_test.APOE_GENOTYPES.csv\",\n",
    "    \"adsp_vars_fin_apoe_unrelated_recode_test.APOE_GENOTYPES.csv\",\n",
    "    \"adsp_vars_mde_apoe_unrelated_recode_test.APOE_GENOTYPES.csv\",\n",
    "    \"adsp_vars_sas_apoe_unrelated_recode_test.APOE_GENOTYPES.csv\"\n",
    "]\n",
    "\n",
    "# Combined dataframe to store all results\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and process\n",
    "for file in files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Filter the rows based on the APOE_GENOTYPE\n",
    "    filtered_df = df[df['APOE_GENOTYPE'].isin(['{APOE genotype}'])]  # for e4 carriers: {APOE genotype} ='e3/e4', 'e4/e4', 'e1/e4' # for e4e4 carriers: {APOE genotype}='e4/e4' # for e3e3 carriers: {APOE genotype}='e3e3'\n",
    "    \n",
    "    # Keep only FID, IID, and APOE_GENOTYPE columns\n",
    "    filtered_df = filtered_df[['FID', 'IID', 'APOE_GENOTYPE']]\n",
    "    \n",
    "    # Save the filtered results for each ancestry\n",
    "    ancestry = file.split('_')[2]  # Extract ancestry from the file name (e.g., 'aac', 'afr')\n",
    "    output_file = os.path.join(output_dir, f\"APOE_filtered_genotypes_{APOE}_{ancestry}.csv\")\n",
    "    filtered_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Add ancestry column to the filtered dataframe for combination later\n",
    "    filtered_df['Ancestry'] = ancestry\n",
    "    \n",
    "    # Append to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, filtered_df], ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a new file\n",
    "combined_output_file = os.path.join(output_dir, \"APOE_filtered_genotypes_combined_{APOE}.csv\")\n",
    "combined_df.to_csv(combined_output_file, index=False)\n",
    "\n",
    "print(f\"Filtered results for each ancestry are saved separately, and the combined file is saved as {combined_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b126f29",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "input_file = f\"{WORK_DIR}/APOE_filtered_genotypes_combined_{APOE}.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Group by Ancestry and APOE_GENOTYPE, and count the occurrences\n",
    "counts = df.groupby(['Ancestry', 'APOE_GENOTYPE']).size().reset_index(name='Count')\n",
    "\n",
    "# Display the result\n",
    "print(counts)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "output_file = f\"{WORK_DIR}/APOE_genotype_counts_{APOE}.csv\"\n",
    "counts.to_csv(output_file, index=False)\n",
    "print(f\"Counts saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d52ab0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "covar_file = f\"{WORK_DIR}/covars_alldata_with999forAGRandRACE_PCA.txt\"\n",
    "apoe_file = f\"{WORK_DIR}/APOE_filtered_genotypes_combined_{APOE}.csv\"\n",
    "output_file = f\"{WORK_DIR}/covars_alldata_with999forAGRandRACE_PCA_{APOE}.txt\"\n",
    "\n",
    "# Load the covariate data\n",
    "covar_df = pd.read_csv(covar_file, sep=\"\\t\")\n",
    "\n",
    "# Load the APOE genotype data\n",
    "apoe_df = pd.read_csv(apoe_file)\n",
    "\n",
    "# Merge the two dataframes on FID and IID (inner merge to keep only matching records)\n",
    "merged_df = covar_df.merge(apoe_df[['FID', 'IID']], on=['FID', 'IID'], how='left', indicator=True)\n",
    "\n",
    "# Create the APOE_STATUS column: 1 if the record exists in the APOE genotype file, otherwise 0 for e4 carriers. \n",
    "# Create the APOE_STATUS column: 2 if the record exists in the APOE genotype file, otherwise 0 for e4e4 carriers. \n",
    "# Create the APOE_STATUS column: 2 if the record exists in the APOE genotype file, otherwise 0 for e3e3 carriers.\n",
    "merged_df['APOE_STATUS'] = merged_df['_merge'].apply(lambda x: 1 if x == 'both' else 0)\n",
    "\n",
    "# Drop the _merge column (it was used to track matching records)\n",
    "merged_df.drop(columns=['_merge'], inplace=True)\n",
    "\n",
    "# Save the result to the output file\n",
    "merged_df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"File saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0581447",
   "metadata": {},
   "source": [
    "#### Logistic regression for conditional model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d77b0683",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%bash\n",
    "module load plink/2.0\n",
    "\n",
    "# Define VCF file\n",
    "vcf_file=\"${WORK_DIR}/vars_${chr}_vcf.vcf.gz\"\n",
    "\n",
    "# Define an array of ancestries\n",
    "ancestries=(\"EUR\" \"AFR\" \"AMR\" \"EAS\" \"SAS\" \"AAC\" \"MDE\" \"AJ\" \"FIN\" \"CAS\" \"CAH\")\n",
    "\n",
    "# Define the output directory\n",
    "output_dir=\"${WORK_DIR}/${APOE}\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "mkdir -p \"$output_dir\"\n",
    "\n",
    "# Loop over each ancestry\n",
    "for ancestry in \"${ancestries[@]}\"; do\n",
    "  echo \"Processing ancestry: $ancestry\"\n",
    "\n",
    "  # Run PLINK\n",
    "  plink2 \\\n",
    "    --vcf \"$vcf_file\" \\\n",
    "    --double-id \\\n",
    "    --pheno \"/${WORK_DIR}/FID_IID_PHENO_${ancestry}.fam\" \\\n",
    "    --adjust \\\n",
    "    --ci 0.95 \\\n",
    "    --covar \"/${WORK_DIR}/covars_alldata_with999forAGRandRACE_PCA_${APOE}.txt\" \\\n",
    "    --covar-name SEX,AGE,PC1,PC2,PC3,PC4,PC5,APOE_STATUS \\\n",
    "    --threads 15 \\\n",
    "    --covar-variance-standardize \\\n",
    "    --out \"${output_dir}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_${chr}_${APOE}\" \\\n",
    "    --glm omit-ref firth-fallback cols=+a1freq,+a1freqcc,+a1count,+totallele,+a1countcc,+totallelecc,+gcountcc,+err \\\n",
    "    --silent\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4b240",
   "metadata": {},
   "source": [
    "#### Preparing tables for the results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f588c8c4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%bash\n",
    "# Define the output file name\n",
    "output_file=\"${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_All_variants_${APOE}.txt\"\n",
    "\n",
    "# Combine the files into one\n",
    "cat ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr11_${APOE}.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr14_${APOE}.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr15_${APOE}.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr16_${APOE}.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr17_${APOE}.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr19_${APOE}.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr20_${APOE}.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr21_${APOE}.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr2_${APOE}.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr4_${APOE}.PHENO1.glm.logistic.hybrid \\\n",
    "    ${WORK_DIR}/Logistic_FID_IID_PHENO_case_controls_${ancestry}_vars_chr7_${APOE}.PHENO1.glm.logistic.hybrid > \"$output_file\"\n",
    "\n",
    "echo \"Combined file created: $output_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbd254",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "dir_path = f\"{WORK_DIR}\"\n",
    "\n",
    "# Define the list of files and their corresponding ancestry codes\n",
    "files_and_ancestries = [\n",
    "    (f\"Logistic_FID_IID_PHENO_case_controls_AAC_All_variants_{APOE}.txt\", \"AAC\"),\n",
    "    (f\"Logistic_FID_IID_PHENO_case_controls_AFR_All_variants_{APOE}.txt\", \"AFR\"),\n",
    "    (f\"Logistic_FID_IID_PHENO_case_controls_AMR_All_variants_{APOE}.txt\", \"AMR\"),\n",
    "    (f\"Logistic_FID_IID_PHENO_case_controls_AJ_All_variants_{APOE}.txt\", \"AJ\"),\n",
    "    (f\"Logistic_FID_IID_PHENO_case_controls_EUR_All_variants_{APOE}.txt\", \"EUR\"),\n",
    "    (f\"Logistic_FID_IID_PHENO_case_controls_CAS_All_variants_{APOE}.txt\", \"CAS\"),\n",
    "    (f\"Logistic_FID_IID_PHENO_case_controls_SAS_All_variants_{APOE}.txt\", \"SAS\"),\n",
    "    (f\"Logistic_FID_IID_PHENO_case_controls_MDE_All_variants_{APOE}.txt\", \"MDE\"),\n",
    "    (f\"Logistic_FID_IID_PHENO_case_controls_EAS_All_variants_{APOE}.txt\", \"EAS\"),\n",
    "    (f\"Logistic_FID_IID_PHENO_case_controls_FIN_All_variants_{APOE}.txt\", \"FIN\"),\n",
    "    (f\"Logistic_FID_IID_PHENO_case_controls_CAH_All_variants_{APOE}.txt\", \"CAH\"),\n",
    "    \n",
    "]\n",
    "\n",
    "# Process each file\n",
    "for file_name, ancestry in files_and_ancestries:\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "    # Add the new column with the corresponding ancestry code\n",
    "    df['ancestry'] = ancestry\n",
    "\n",
    "    # Save the updated DataFrame to a new file\n",
    "    output_file_name = file_name.replace(\".txt\", \"_with_ancestry.txt\")\n",
    "    output_file_path = os.path.join(dir_path, output_file_name)\n",
    "    df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "    print(f\"Updated file saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f1a7a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "dir_path = f\"{WORK_DIR}\"\n",
    "\n",
    "# List of file paths to be combined\n",
    "file_paths = [\n",
    "    f\"Logistic_FID_IID_PHENO_case_controls_EAS_All_variants_{APOE}_with_ancestry.txt\",\n",
    "    f\"Logistic_FID_IID_PHENO_case_controls_SAS_All_variants_{APOE}_with_ancestry.txt\",\n",
    "    f\"Logistic_FID_IID_PHENO_case_controls_CAS_All_variants_{APOE}_with_ancestry.txt\",\n",
    "    f\"Logistic_FID_IID_PHENO_case_controls_EUR_All_variants_{APOE}_with_ancestry.txt\",\n",
    "    f\"Logistic_FID_IID_PHENO_case_controls_AMR_All_variants_{APOE}_with_ancestry.txt\",\n",
    "    f\"Logistic_FID_IID_PHENO_case_controls_AAC_All_variants_{APOE}_with_ancestry.txt\",\n",
    "    f\"Logistic_FID_IID_PHENO_case_controls_AFR_All_variants_{APOE}_with_ancestry.txt\",\n",
    "    f\"Logistic_FID_IID_PHENO_case_controls_AJ_All_variants_{APOE}_with_ancestry.txt\",\n",
    "    f\"Logistic_FID_IID_PHENO_case_controls_MDE_All_variants_{APOE}_with_ancestry.txt\",\n",
    "    f\"Logistic_FID_IID_PHENO_case_controls_CAH_All_variants_{APOE}_with_ancestry.txt\",\n",
    "    f\"Logistic_FID_IID_PHENO_case_controls_FIN_All_variants_{APOE}_with_ancestry.txt\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to hold the DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Read each file and append the DataFrame to the list\n",
    "for file_name in file_paths:\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new file\n",
    "output_file_path = os.path.join(dir_path, f\"Combined_Logistic_{APOE}_with_ancestry.hybrid\")\n",
    "combined_df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Combined data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b52afb",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined data file\n",
    "file_path = f\"{WORK_DIR}/Combined_Logistic_{APOE}_with_ancestry.hybrid\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Filter the rows where the TEST column has the value \"ADD\"\n",
    "filtered_df = df[df['TEST'] == 'ADD']\n",
    "\n",
    "# Save the filtered DataFrame to a new file\n",
    "output_file_path = f\"{WORK_DIR}/Filtered_Combined_Logistic_{APOE}_with_ancestry.hybrid\"\n",
    "filtered_df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac5d49",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the filtered data file\n",
    "file_path = f\"{WORK_DIR}/Filtered_Combined_Logistic_{APOE}_with_ancestry.hybrid\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Select only the required columns\n",
    "columns_to_keep = ['#CHROM', 'POS', 'REF', 'ALT', 'A1', 'P', 'OR', 'L95', 'U95', 'ancestry']\n",
    "filtered_df = df[columns_to_keep]\n",
    "\n",
    "# Save the filtered DataFrame to a new file\n",
    "output_file_path = f\"{WORK_DIR}/Selected_Columns_Filtered_Combined_Logistic_{APOE}_with_ancestry.hybrid\"\n",
    "filtered_df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Data with selected columns saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e2bfd",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e3d48",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data file\n",
    "file_path = f\"{WORK_DIR}/Selected_Columns_Filtered_Combined_Logistic_{APOE}_with_ancestry.hybrid\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Define the desired order of ancestries\n",
    "ancestry_order = ['EUR', 'AFR', 'AMR', 'EAS', 'SAS', 'MDE', 'AJ', 'FIN', 'AAC', 'CAS', 'CAH']\n",
    "\n",
    "# Initialize a list to hold the rows for the new table\n",
    "combined_data = []\n",
    "\n",
    "# Group the data by variation (defined by #CHROM, POS, REF, ALT)\n",
    "grouped = df.groupby(['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "\n",
    "# Iterate over each group and format the data for the combined table\n",
    "for (chrom, pos, ref, alt), group in grouped:\n",
    "    # Append the variation row\n",
    "    combined_data.append([f\"{chrom}:{pos} {ref}>{alt}\", \"\", \"\", \"\"])\n",
    "    \n",
    "    # Append the header row for ancestries\n",
    "    combined_data.append([\"Ancestry\", \"A1\", \"P\", \"OR (L95_U95)\"])\n",
    "    \n",
    "    # Create a dictionary of ancestries and their data\n",
    "    ancestry_dict = {row['ancestry']: [row['A1'], row['P'], f\"{row['OR']} ({row['L95']}_{row['U95']})\"] for _, row in group.iterrows()}\n",
    "    \n",
    "    # Append the data rows for each ancestry in the specified order if it exists\n",
    "    for ancestry in ancestry_order:\n",
    "        if ancestry in ancestry_dict:\n",
    "            combined_data.append([ancestry] + ancestry_dict[ancestry])\n",
    "\n",
    "    # Add an empty row for separation between variations\n",
    "    combined_data.append([\"\", \"\", \"\", \"\"])\n",
    "\n",
    "# Convert the combined data into a DataFrame\n",
    "combined_df = pd.DataFrame(combined_data)\n",
    "\n",
    "# Save the combined DataFrame to a new file\n",
    "output_file_path = f\"Combined_Variations_Table_{APOE}.tsv\"\n",
    "combined_df.to_csv(output_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "print(f\"Combined variations table saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499abfd",
   "metadata": {},
   "source": [
    "#### Create three covariate files that include APOE status (e4 carriers, e4/e4 carriers, and e3/e3 carriers) and protective/resilient variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd31960",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import vcf  \n",
    "\n",
    "# Load the covariate file\n",
    "covar_file = f'{WORK_DIR}/covars_alldata_with999forAGRandRACE_PCAf_{APOE}.txt'\n",
    "covar_df = pd.read_csv(covar_file, sep='\\t')\n",
    "\n",
    "# Set FID as index for easy matching\n",
    "covar_df.set_index('FID', inplace=True)\n",
    "\n",
    "# List of VCF files for all chromosomes\n",
    "vcf_files = [\n",
    "    f'{WORK_DIR}/vars_chr2_vcf.vcf',\n",
    "    f'{WORK_DIR}/vars_chr19_vcf.vcf',\n",
    "    f'{WORK_DIR}/vars_chr7_vcf.vcf',\n",
    "    f'{WORK_DIR}/vars_chr21_vcf.vcf',\n",
    "    f'{WORK_DIR}/vars_chr4_vcf.vcf',\n",
    "    f'{WORK_DIR}/vars_chr11_vcf.vcf',\n",
    "    f'{WORK_DIR}/vars_chr14_vcf.vcf',\n",
    "    f'{WORK_DIR}/vars_chr20_vcf.vcf',\n",
    "    f'{WORK_DIR}/vars_chr15_vcf.vcf',\n",
    "    f'{WORK_DIR}/vars_chr16_vcf.vcf',\n",
    "    f'{WORK_DIR}/vars_chr17_vcf.vcf'\n",
    "]\n",
    "\n",
    "# Initialize a dictionary to store genotype data\n",
    "genotype_data = {}\n",
    "\n",
    "# Function to extract genotypes from a VCF file\n",
    "def extract_genotypes(vcf_file):\n",
    "    vcf_reader = vcf.Reader(open(vcf_file, 'r'))\n",
    "    for record in vcf_reader:\n",
    "        variant_id = record.ID\n",
    "        if variant_id not in genotype_data:\n",
    "            genotype_data[variant_id] = {}\n",
    "\n",
    "        for sample in record.samples:\n",
    "            fid = sample.sample  # Sample ID, which matches FID\n",
    "            genotype = sample['GT']  # Genotype (e.g., 0/1, 1/1, etc.)\n",
    "            genotype_data[variant_id][fid] = genotype\n",
    "\n",
    "# Process each VCF file and extract genotypes\n",
    "for vcf_file in vcf_files:\n",
    "    extract_genotypes(vcf_file)\n",
    "\n",
    "# Add genotype data to the covariate DataFrame\n",
    "for variant_id, genotypes in genotype_data.items():\n",
    "    # Create a new column for each variant in the covariate dataframe\n",
    "    covar_df[variant_id] = covar_df.index.map(genotypes)\n",
    "\n",
    "# Save the updated DataFrame to a new file\n",
    "output_file = f'{WORK_DIR}/covars_alldata_with999forAGRandRACE_PCA_{APOE}_allvariants.txt'\n",
    "covar_df.to_csv(output_file, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5928f2a2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the covariate file\n",
    "covar_file = f'{WORK_DIR}/covars_alldata_with999forAGRandRACE_PCA_{APOE}_allvariants.txt'\n",
    "covar_df = pd.read_csv(covar_file, sep='\\t')\n",
    "\n",
    "# Function to map genotype codes to integers\n",
    "def map_genotype(genotype):\n",
    "    if genotype == '0/0' or genotype == './.':\n",
    "        return 0\n",
    "    elif genotype == '0/1':\n",
    "        return 1\n",
    "    elif genotype == '1/1':\n",
    "        return 2\n",
    "    else:\n",
    "        return None  # For any unknown genotype\n",
    "\n",
    "# List of genotype columns to transform\n",
    "genotype_columns = [chr{}:position:A1:A2]\n",
    "\n",
    "# Apply the mapping function to the genotype columns\n",
    "for column in genotype_columns:\n",
    "    if column in covar_df.columns:  # Check if the column exists in the DataFrame\n",
    "        covar_df[column] = covar_df[column].apply(map_genotype)\n",
    "\n",
    "# Save the updated DataFrame to a new file\n",
    "output_file = f'{WORK_DIR}/covars_alldata_with999forAGRandRACE_PCA_{APOE}_allvariants_mapped.txt'\n",
    "covar_df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(\"Genotype mapping completed and saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99418d6c",
   "metadata": {},
   "source": [
    "#### Separate covariate files based on ancestry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7bc318",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the covariate file\n",
    "covar_file = f'{WORK_DIR}/covars_alldata_with999forAGRandRACE_PCA_{APOE}_allvariants_mapped.txt'\n",
    "covar_df = pd.read_csv(covar_file, sep='\\t')\n",
    "\n",
    "# List of ancestries\n",
    "ancestries = [\"EUR\", \"AFR\", \"AMR\", \"EAS\", \"SAS\", \"AAC\", \"MDE\", \"AJ\", \"FIN\", \"CAS\", \"CAH\"]\n",
    "\n",
    "# Create a new directory to save the results\n",
    "output_directory = f'{WORK_DIR}'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to filter based on ancestry\n",
    "def filter_by_ancestry(ancestry):\n",
    "    # Construct the filename for the specified ancestry\n",
    "    fam_file = f'{WORK_DIR}/FID_IID_PHENO_{ancestry}.fam'\n",
    "    # Read the FID from the fam file\n",
    "    ancestry_df = pd.read_csv(fam_file, sep=' ', header=None, names=['FID', 'IID', 'PHENO1'])\n",
    "    \n",
    "    # Filter the covariate DataFrame to keep only the FIDs present in the ancestry DataFrame\n",
    "    filtered_df = covar_df[covar_df['FID'].isin(ancestry_df['FID'])]\n",
    "    \n",
    "    # Define the output file name in the new directory\n",
    "    output_file = f'{output_directory}/covars_alldata_with999forAGRandRACE_PCA_{APOE}_allvariants_mapped_{ancestry}.txt'\n",
    "    \n",
    "    # Save the filtered DataFrame to a new file\n",
    "    filtered_df.to_csv(output_file, sep='\\t', index=False)\n",
    "    print(f\"Filtered file saved for {ancestry}: {output_file}\")\n",
    "\n",
    "# Loop through each ancestry and filter the data\n",
    "for ancestry in ancestries:\n",
    "    filter_by_ancestry(ancestry)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
